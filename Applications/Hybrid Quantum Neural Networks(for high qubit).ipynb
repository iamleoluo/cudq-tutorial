{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the relevant packages.\n",
    "\n",
    "! pip install numpy==1.24.4 matplotlib==3.8.4 torch==2.0.1+cu118 torchvision==0.15.2+cu118 scikit-learn==1.4.2 -q --extra-index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "torch.manual_seed(22)\n",
    "cudaq.set_random_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDAQ and PyTorch to run on either CPU or GPU.\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "#cudaq.set_target(\"qpp-cpu\")\n",
    "\n",
    "cudaq.set_target(\"tensornet\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(target_digits, sample_count, test_size):\n",
    "    \"\"\"Load and prepare the MNIST dataset to be used\n",
    "\n",
    "    Args:\n",
    "        target_digits (list): digits to perform classification of\n",
    "        sample_count (int): total number of images to be used\n",
    "        test_size (float): percentage of sample_count to be used as test set, the remainder is the training set\n",
    "\n",
    "    Returns:\n",
    "        dataset in train, test format with targets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "    dataset = datasets.MNIST(\"./data\",\n",
    "                             train=True,\n",
    "                             download=True,\n",
    "                             transform=transform)\n",
    "\n",
    "    # Filter out the required labels.\n",
    "    idx = (dataset.targets == target_digits[0]) | (dataset.targets\n",
    "                                                   == target_digits[1])\n",
    "    dataset.data = dataset.data[idx]\n",
    "    dataset.targets = dataset.targets[idx]\n",
    "\n",
    "    # Select a subset based on number of datapoints specified by sample_count.\n",
    "    subset_indices = torch.randperm(dataset.data.size(0))[:sample_count]\n",
    "\n",
    "    x = dataset.data[subset_indices].float().unsqueeze(1).to(device)\n",
    "\n",
    "    y = dataset.targets[subset_indices].to(device).float().to(device)\n",
    "\n",
    "    # Relabel the targets as a 0 or a 1.\n",
    "    y = torch.where(y == min(target_digits), 0.0, 1.0)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                        y,\n",
    "                                                        test_size=test_size /\n",
    "                                                        100,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical parameters.\n",
    "\n",
    "sample_count = 3000  # Total number of images to use.\n",
    "target_digits = [5, 6]  # Hand written digits to classify.\n",
    "test_size = 30  # Percentage of dataset to be used for testing.\n",
    "classification_threshold = 0.5  # Classification boundary used to measure accuracy.\n",
    "epochs = 2  # Number of epochs to train for.\n",
    "\n",
    "# Quantum parmeters.\n",
    "#NEW FOR GREATER COMPUTATE CAPACITY\n",
    "qubit_count = 10  # Number of qubits to use.\n",
    "hamiltonian = spin.z(0)  # Measurement operator.\n",
    "shift = torch.tensor(torch.pi / 2)  # Magnitude of parameter shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = prepare_data(target_digits, sample_count,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some images from the training set to visualise.\n",
    "if device != 'cpu':\n",
    "    sample_to_plot = x_train[:10].to(torch.device('cpu'))\n",
    "else:\n",
    "    sample_to_plot = x_train[:10]\n",
    "\n",
    "grid_img = torchvision.utils.make_grid(sample_to_plot,\n",
    "                                       nrow=5,\n",
    "                                       padding=3,\n",
    "                                       normalize=True)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumFunction(Function):\n",
    "    \"\"\"Allows the quantum circuit to input data, output expectation values\n",
    "    and calculate gradients of variational parameters via finite difference\"\"\"\n",
    "\n",
    "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
    "        \"\"\"Define the quantum circuit in CUDA Quantum\"\"\"\n",
    "        \n",
    "        # NEW FOR GREATER COMPUTATE CAPACITY\n",
    "        # Define the quantum circuit in CUDA Quantum\n",
    "        @cudaq.kernel\n",
    "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
    "            qubits = cudaq.qvector(qubit_count)\n",
    "            \n",
    "            ry(thetas[0], qubits[0])\n",
    "            rx(thetas[1], qubits[0])\n",
    "\n",
    "        self.kernel = kernel\n",
    "        self.qubit_count = qubit_count\n",
    "        self.hamiltonian = hamiltonian\n",
    "\n",
    "    def run(self, theta_vals: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Excetute the quantum circuit to output an expectation value\"\"\"\n",
    "\n",
    "        #If running on GPU, thetas is a torch.tensor that will live on GPU memory. The observe function calls a .tolist() method on icputs which moves thetas from GPU to CPU.\n",
    "\n",
    "        qubit_count = [self.qubit_count for _ in range(theta_vals.shape[0])]\n",
    "\n",
    "        results = cudaq.observe(self.kernel, self.hamiltonian, qubit_count,\n",
    "                                theta_vals)\n",
    "\n",
    "        exp_vals = [results[i].expectation() for i in range(len(results))]\n",
    "        exp_vals = torch.Tensor(exp_vals).to(device)\n",
    "\n",
    "        return exp_vals\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, thetas: torch.tensor, quantum_circuit,\n",
    "                shift) -> torch.tensor:\n",
    "\n",
    "        # Save shift and quantum_circuit in context to use in backward.\n",
    "        ctx.shift = shift\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        # Calculate expectation value.\n",
    "        exp_vals = ctx.quantum_circuit.run(thetas).reshape(-1, 1)\n",
    "\n",
    "        ctx.save_for_backward(thetas, exp_vals)\n",
    "\n",
    "        return exp_vals\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Backward pass computation via finite difference\"\"\"\n",
    "\n",
    "        thetas, _ = ctx.saved_tensors\n",
    "\n",
    "        gradients = torch.zeros(thetas.shape, device=device)\n",
    "\n",
    "        for i in range(thetas.shape[1]):\n",
    "\n",
    "            thetas_plus = thetas.clone()\n",
    "            thetas_plus[:, i] += ctx.shift\n",
    "            exp_vals_plus = ctx.quantum_circuit.run(thetas_plus)\n",
    "\n",
    "            thetas_minus = thetas.clone()\n",
    "            thetas_minus[:, i] -= ctx.shift\n",
    "            exp_vals_minus = ctx.quantum_circuit.run(thetas_minus)\n",
    "\n",
    "            gradients[:, i] = (exp_vals_plus - exp_vals_minus) / (2 * ctx.shift)\n",
    "\n",
    "        gradients = torch.mul(grad_output, gradients)\n",
    "\n",
    "        return gradients, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\"Encapsulates a quantum circuit into a quantum layer adhering PyTorch convention\"\"\"\n",
    "\n",
    "    def __init__(self, qubit_count: int, hamiltonian, shift: torch.tensor):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "\n",
    "        self.quantum_circuit = QuantumFunction(qubit_count, hamiltonian)\n",
    "        self.shift = shift\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        result = QuantumFunction.apply(input, self.quantum_circuit, self.shift)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_QNN(nn.Module):\n",
    "    \"\"\"Structure of the hybrid neural network with classical fully connected layers and quantum layers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Hybrid_QNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # The 2 outputs from PyTorch fc5 layer feed into the 2 variational gates in the quantum circuit.\n",
    "        self.quantum = QuantumLayer(qubit_count, hamiltonian, shift)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 28 * 28)  # Turns images into vectors.\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Quantum circuit outputs an expectation value which is fed into the sigmoid activation function to perform classification.\n",
    "        x = torch.sigmoid(self.quantum(x))\n",
    "\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def accuracy_score(y, y_hat):\n",
    "    return sum((y == (y_hat >= classification_threshold))) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = Hybrid_QNN().to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(hybrid_model.parameters(),\n",
    "                           lr=0.001,\n",
    "                           weight_decay=0.8)\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "\n",
    "training_cost = []\n",
    "testing_cost = []\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "\n",
    "hybrid_model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat_train = hybrid_model(x_train).to(device)\n",
    "\n",
    "    train_cost = loss_function(y_hat_train, y_train).to(device)\n",
    "\n",
    "    train_cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    training_accuracy.append(accuracy_score(y_train, y_hat_train))\n",
    "    training_cost.append(train_cost.item())\n",
    "\n",
    "    hybrid_model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        y_hat_test = hybrid_model(x_test).to(device)\n",
    "\n",
    "        test_cost = loss_function(y_hat_test, y_test).to(device)\n",
    "\n",
    "        testing_accuracy.append(accuracy_score(y_test, y_hat_test))\n",
    "        testing_cost.append(test_cost.item())\n",
    "    print(f\"Epoch: {epoch + 1}, Training Cost: {train_cost.item()}, Testing Cost: {test_cost.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_cost, label='Train')\n",
    "plt.plot(testing_cost, label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "training_accuracy = np.array(torch.tensor(training_accuracy).cpu())\n",
    "testing_accuracy = np.array(torch.tensor(testing_accuracy).cpu())\n",
    "plt.plot(training_accuracy, label='Train')\n",
    "plt.plot(testing_accuracy, label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperquantum-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
